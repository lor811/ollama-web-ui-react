# ollama-web-ui-react

A simple full-stack React+FastAPI-based web interface for interacting with [ollama](https://ollama.com/)

## üöÄ Features

- üîå Uses ollama API to interact with the server hosted locally on your machine
- üí¨ Chat UI with streaming response
- üéõÔ∏è Model selection and context management
- üß† Supports [multiple models](https://ollama.com/search)

You must have [ollama](https://ollama.com/) installed on your local machine, serving a local server, and at least one model installed to run the application.
Once you installed ollama, run the server:
```sh
ollama serve
```
and download at least one model using:
```sh
ollama pull {model}
```

For example:
```sh
ollama pull llama3.2:1b
```

---

## üõ†Ô∏è Installation

First, clone the repository and go to the project root:
```sh
git clone https://github.com/lor811/ollama-web-ui-react.git
cd ollama-web-ui-react
```

### 1. On macOS

**1. Install dependencies**
```sh
cd frontend
npm install
```
**2. Build the frontend project**
```sh
npm run build
```

**3. Setup python envinronment**
```sh
cd ../backend
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
**4. Run FastAPI**
```sh
cd src
uvicorn main:app --host 127.0.0.1 --port 8000
```

The app should now be available at http://localhost:8000

### 2. On Windows

**1. Install dependencies**
```powershell
cd frontend
npm install
```
**2. Build the frontend project**
```powershell
npm run build
```

**3. Setup python envinronment**
```powershell
cd ..\backend
python -m venv .venv
.venv\Scripts\activate.bat
pip install -r requirements.txt
```
**4. Run FastAPI**
```powershell
cd src
uvicorn main:app --host 127.0.0.1 --port 8000
```

The app should now be available at http://localhost:8000